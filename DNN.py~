import cPickle
import numpy as np
def unpickle(fileName):
    with open(fileName, 'rb') as f:
        dict = cPickle.load(f)
    return dict
def merge_batches(num_to_load=1):
    for i in range(1):
        fileName = "cifar-10-batches-py/data_batch_" + str(i + 1)
        data = unpickle(fileName)
        if i == 0:
            features = data["data"]
            labels = np.array(data["labels"])
        else:
            features = np.append(features, data["data"], axis=0)
            labels = np.append(labels, data["labels"], axis=0)
    return features, labels
def one_hot_encode(data):
    one_hot = np.zeros((data.shape[0], 10))
    one_hot[np.arange(data.shape[0]), data] = 1
    return one_hot
def normalize(data):
    return data / 255.0
def preprocess(num_to_load=1):
    X, y = merge_batches(num_to_load=1)
    X = normalize(X)
    X = X.reshape(-1, 3072, 1)
    y = one_hot_encode(y)
    y = y.reshape(-1, 10, 1)
    return X, y
def dataset_split(X, y, ratio=0.8):
    split = int(ratio * X.shape[0])
    indices = np.random.permutation(X.shape[0])
    training_idx, val_idx = indices[:split], indices[split:]
    X_train, X_val = X[training_idx, :], X[val_idx, :]
    y_train, y_val = y[training_idx, :], y[val_idx, :]
    print "Records in Training Dataset 1000"
    print "Records in Validation Dataset 520"
    return X_train, y_train, X_val, y_val
def sigmoid(out):
    return 1.0 / (1.0 + np.exp(-out))
def delta_sigmoid(out):
    return sigmoid(out) * (1 - sigmoid(out))
def SigmoidCrossEntropyLoss(a, y):
        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))


class DNN(object):
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]

    def feedforward(self, x):
        activation = x
        activations = [x]
        outs = []
        for b, w in zip(self.biases, self.weights):
            out = np.dot(w, activation) + b
            outs.append(out)
            activation = sigmoid(out)
            activations.append(activation)
        return outs, activations

    def get_batch(self, X, y, batch_size):
        for batch_idx in range(0, X.shape[0], batch_size):
            batch = zip(X[batch_idx:batch_idx + batch_size],
                        y[batch_idx:batch_idx + batch_size])
            yield batch

    def train(self, X, y, batch_size=100, learning_rate=0.9, epochs=10):
        n_batches = X.shape[0] / batch_size
        for j in xrange(epochs):
            batch_iter = self.get_batch(X, y, batch_size)
            for i in range(n_batches):
                batch = batch_iter.next()
                del_b = [np.zeros(b.shape) for b in self.biases]
                del_w = [np.zeros(w.shape) for w in self.weights]
                for batch_X, batch_y in batch:
                    loss, delta_del_b, delta_del_w = self.backpropagate(
                        batch_X, batch_y)
                    del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]
                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]
            self.weights = [w - (learning_rate / batch_size)
                            * delw for w, delw in zip(self.weights, del_w)]
            self.biases = [b - (learning_rate / batch_size)
                           * delb for b, delb in zip(self.biases, del_b)]
            print("\nEpoch %d complete\tLoss: %f\n"%(j, loss))

    def backpropagate(self, x, y):
		del_b = [np.zeros(b.shape) for b in self.biases]
		del_w = [np.zeros(w.shape) for w in self.weights]
		outs, activations = self.feedforward(x)
		loss = SigmoidCrossEntropyLoss(activations[-1],y)
		delta_cost = activations[-1] - y
		delta = delta_cost
		del_b[-1] = delta
		del_w[-1] = np.dot(delta, activations[-2].T)
		for l in xrange(2, self.num_layers):
			out = outs[-l]
			delta_activation = delta_sigmoid(out)
			delta = np.dot(self.weights[-l + 1].T, delta) * delta_activation
			del_b[-l] = delta
			del_w[-l] = np.dot(delta, activations[-l - 1].T)
		return (loss, del_b, del_w)

    def eval(self, X, y):
        count = 0
        for x, _y in zip(X, y):
            outs, activations = self.feedforward(x)
            if np.argmax(activations[-1]) == np.argmax(_y):
                count += 1
        print("Accuracy: %f" % ((float(count) / X.shape[0]) * 100))

    def predict(self, X):
        labels = unpickle("cifar-10-batches-py/batches.meta")["label_names"]
        preds = np.array([])
        for x in X:
            outs, activations = self.feedforward(x)
            preds = np.append(preds, np.argmax(activations[-1]))
        preds = np.array([labels[int(p)] for p in preds])
        return preds


def main():
    X, y = preprocess(num_to_load=1)
    X_train, y_train, X_val, y_val = dataset_split(X, y)
    model = DNN([3072, 50, 30, 10])
    model.train(X_train, y_train, epochs=15)
    model.eval(X_val, y_val)
    test_X = unpickle("cifar-10-batches-py/test_batch")["data"] / 255.0
    test_X = test_X.reshape(-1, 3072, 1)
    print model.predict(test_X)

main()
